{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e568c94",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms\n",
    "\n",
    "- The reasons for using attention mechanisms in neural networks\n",
    "- A basic self-attention framework, progressing to an enhanced self-attention mechanism \n",
    "- A causal attention module that allows LLMs to generate one token at a time\n",
    "- Masking randomly selected attention weights with dropout to reduce overfitting\n",
    "- Stacking multiple causal attention modules into a multi-head attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bb970",
   "metadata": {},
   "source": [
    "Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder‚Äìdecoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9741434",
   "metadata": {},
   "source": [
    "In an encoder‚Äìdecoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8e400",
   "metadata": {},
   "source": [
    "The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a7cca",
   "metadata": {},
   "source": [
    "Before the advent of transformer models, encoder‚Äìdecoder RNNs were a popular choice for machine translation. The encoder takes a sequence of tokens from the source language as input, where a hidden state (an intermediate neural network layer) of the encoder encodes a compressed representation of the entire input sequence. Then, the decoder uses its current hidden state to begin the translation, token by token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fafa5e",
   "metadata": {},
   "source": [
    "While we don‚Äôt need to know the inner workings of these encoder‚Äìdecoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea0609",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or ‚Äúattend to,‚Äù all other positions in the same sequence when computing the representation of a sequence. Self-attention is a key component of contemporary LLMs based on the transformer architecture, such as the GPT series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d373e9",
   "metadata": {},
   "source": [
    "# The meaning of \"self\"\n",
    "\n",
    "In self-attention, \"self\" refers to computing attention **within the same sequence**. Specifically:\n",
    "\n",
    "- Each element in the sequence establishes relationships with **all other elements in that same sequence** (including itself)\n",
    "- For example: when processing a sentence, each word attends to all other words in that sentence\n",
    "- \"Self\" emphasizes **attending to itself**, meaning the relationships are computed among elements within the input sequence itself\n",
    "\n",
    "**Example**: The sentence \"I love eating apples\"\n",
    "- The word \"apples\" will attend to \"I\", \"love\", \"eating\", and \"apples\" itself\n",
    "- All these relationships are established **within the same** input sentence\n",
    "\n",
    "## What are \"traditional attention mechanisms\"?\n",
    "\n",
    "Traditional attention mechanisms primarily refer to **attention used in sequence-to-sequence models**:\n",
    "\n",
    "- Attention is computed **between two different sequences**\n",
    "- Typical application: machine translation\n",
    "  - **Encoder sequence** (source language): English sentence\n",
    "  - **Decoder sequence** (target language): Chinese sentence\n",
    "  - Each Chinese character in the decoder attends to all English words in the encoder\n",
    "\n",
    "**Key difference**:\n",
    "- **Traditional attention**: Establishes relationships between two different sequences (Sequence A ‚Üí Sequence B)\n",
    "- **Self-attention**: Establishes relationships within a single sequence (among elements within Sequence A itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418aa30b",
   "metadata": {},
   "source": [
    "# A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf397fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     (x^4)\n",
    "    [0.77, 0.25, 0.10], # one      (x^5)\n",
    "    [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3c389",
   "metadata": {},
   "source": [
    "![attention-score](../images/attention-score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47018e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query vector: tensor([0.5500, 0.8700, 0.6600])\n",
      "inputs shape: 6\n",
      "attn_scores_2: tensor([0., 0., 0., 0., 0., 0.])\n",
      "attn_scores_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # the second input token served as query (journey)\n",
    "print(\"query vector:\", query)\n",
    "\n",
    "print(\"inputs shape:\", inputs.shape[0])\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(\"attn_scores_2:\", attn_scores_2)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"attn_scores_2:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5cee6",
   "metadata": {},
   "source": [
    "the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or ‚Äúattends to,‚Äù any other element: the higher the dot product, the higher the similarity and attention score between two elements.\n",
    "\n",
    "Raku code to compute attention scores using dot products:\n",
    "\n",
    "```raku\n",
    "# ÂÆö‰πâËæìÂÖ•Âº†ÈáèÔºà6‰∏™tokenÔºåÊØè‰∏™3Áª¥ÂêëÈáèÔºâ\n",
    "my @inputs = (\n",
    "    [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "    [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "    [0.05, 0.80, 0.55]   # step     (x^6)\n",
    ");\n",
    "\n",
    "# ÈÄâÊã©Á¨¨‰∫å‰∏™token‰Ωú‰∏∫Êü•ËØ¢ÂêëÈáèÔºàÁ¥¢Âºï‰∏∫1Ôºâ\n",
    "my @query = @inputs[1].flat;  # ‰ΩøÁî® .flat Â±ïÂºÄÊï∞ÁªÑ\n",
    "say \"query vector: [{@query.join(', ')}]\";\n",
    "say \"inputs shape: {@inputs.elems}\";\n",
    "\n",
    "# ÂàõÂª∫Á©∫Êï∞ÁªÑÂ≠òÂÇ®Ê≥®ÊÑèÂäõÂàÜÊï∞ÔºàRaku Êï∞ÁªÑ‰ºöËá™Âä®Êâ©Â±ïÔºâ\n",
    "my @attn_scores_2;\n",
    "say \"attn_scores_2: []\";\n",
    "\n",
    "# ‰ΩøÁî®Ë∂ÖËøêÁÆóÁ¨¶ ¬ª*¬´ Âíå reduction operator [+] ËÆ°ÁÆóÊØè‰∏™ËæìÂÖ•ÂêëÈáè‰∏éÊü•ËØ¢ÂêëÈáèÁöÑÁÇπÁßØ\n",
    "my @attn-scores = @inputs.map: -> @x { [+] @x ¬ª*¬´ @query };\n",
    "say \"attn_scores: [{@attn-scores.join(', ')}]\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe96eef",
   "metadata": {},
   "source": [
    "## normalization\n",
    "\n",
    "we normalize each of the attention scores we computed previously. The main goal behind the normalization is to obtain attention weights that sum up to 1. This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM. Here‚Äôs a straightforward method for achieving this normalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13b165fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "sum of attention weights: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"attention weights:\", attn_weights_2_tmp)\n",
    "print(\"sum of attention weights:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a8030",
   "metadata": {},
   "source": [
    "```raku\n",
    "# ÂΩí‰∏ÄÂåñÊìç‰ΩúÔºöÂ∞ÜÊ≥®ÊÑèÂäõÂàÜÊï∞Èô§‰ª•ÊÄªÂíåÔºåÂæóÂà∞Ê≥®ÊÑèÂäõÊùÉÈáçÔºàÂíå‰∏∫1Ôºâ\n",
    "my $sum = [+] @attn-scores;\n",
    "my @attn_weights_2_tmp = @attn-scores ¬ª/¬ª $sum;\n",
    "say \"attention weights: {@attn_weights_2_tmp}\";\n",
    "say \"sum of attention weights: {[+] @attn_weights_2_tmp}\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289f5ab",
   "metadata": {},
   "source": [
    "In practice, it‚Äôs more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "093fd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights (naive softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum of attention weights (naive softmax): tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"attention weights (naive softmax):\", attn_weights_2_naive)\n",
    "print(\"sum of attention weights (naive softmax):\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e234704",
   "metadata": {},
   "source": [
    "the softmax function ensures that the attention weights are always positive. This makes the output interpretable as probabilities or relative importance, where higher weights indicate greater importance.\n",
    "\n",
    "Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it‚Äôs advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53cc94be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum of attention weights: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"attention weights:\", attn_weights_2)\n",
    "print(\"sum of attention weights:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a0a10",
   "metadata": {},
   "source": [
    "calculating the context vector z(2) by multiplying the embedded input tokens, x(i), with the corresponding attention weights and then summing the resulting vectors. Thus, context vector z(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1471e13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # the second input token served as query (journey)\n",
    "context_vec_2 = torch.zeros(inputs.shape[1])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(\"context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df18789",
   "metadata": {},
   "source": [
    "## Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a88a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores: tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# step1: compute attention scores for all queries\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for x, x_i in enumerate(inputs):\n",
    "    for i, x_j in enumerate(inputs):\n",
    "        attn_scores[x, i] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(\"attn_scores:\", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4bea51",
   "metadata": {},
   "source": [
    "Each element in the tensor represents an attention score between each pair of inputs.\n",
    "\n",
    "When computing the preceding attention score tensor, we used for loops in Python. However, for loops are generally slow, and we can achieve the same results using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "530fcb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores (matrix multiplication): tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(\"attn_scores (matrix multiplication):\", attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1cc82aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights: tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# step2: compute attention weights for all queries\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(\"attn_weights:\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbf57e",
   "metadata": {},
   "source": [
    "In the context of using PyTorch, the dim parameter in functions like `torch.softmax` specifies the dimension of the input tensor along which the function will be computed. By setting `dim=-1`, we are instructing the `softmax` function to apply the normalization along the last dimension of the attn_scores tensor. If attn_scores is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5fd8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 2 sum: 1.0\n",
      "all row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"row 2 sum:\", row_2_sum)\n",
    "print(\"all row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70fcf903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_context_vecs: tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# step3: compute context vectors for all queries\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(\"all_context_vecs:\", all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b47187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous context vector for query 2: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"previous context vector for query 2:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078d565",
   "metadata": {},
   "source": [
    "# Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b59a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]         # the second input element\n",
    "d_in = inputs.shape[1]  # the input embedding size, d_in=3\n",
    "d_out = 2               # the output embedding size, d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21d032ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight matrix Wq, Wk, and Wv\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842a9f6",
   "metadata": {},
   "source": [
    "We set `requires_grad=False` to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set `requires_grad=True` to update these matrices during model training.\n",
    "\n",
    "Next, we compute the query, key, and value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5d037d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_2: tensor([0.4306, 1.4551])\n",
      "key_2: tensor([0.4433, 1.1419])\n",
      "value_2: tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(\"query_2:\", query_2)\n",
    "print(\"key_2:\", key_2)\n",
    "print(\"value_2:\", value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282f55e",
   "metadata": {},
   "source": [
    "Weight parameters vs. attention weights \n",
    "\n",
    "\n",
    "In the weight matrices W, the term ‚Äúweight‚Äù is short for ‚Äúweight parameters,‚Äù the values of a neural network that are optimized during training. This is not to be confused with the attention weights. As we already saw, attention weights determine the extent to which a context vector depends on the different parts of the input (i.e., to what extent the network focuses on different parts of the input). \n",
    "\n",
    "\n",
    "In summary, weight parameters are the fundamental, learned coefficients that define the network‚Äôs connections, while attention weights are dynamic, context-specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74415ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95cb9a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores_22: tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(\"attn_scores_22:\", attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389ef8d",
   "metadata": {},
   "source": [
    "we can generalize this computation to all attention scores via matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a95e4f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores_2: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(\"attn_scores_2:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb12661",
   "metadata": {},
   "source": [
    "As we can see, as a quick check, the second element in the output matches the attn_score_22 we computed previously\n",
    "\n",
    "Now, we want to go from the attention scores to the attention weights, as illustrated in figure 3.16. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e4770",
   "metadata": {},
   "source": [
    "After computing the attention scores œâ, \n",
    "the next step is to normalize these scores using the softmax function to obtain the attention weights ùõº.\n",
    "\n",
    "![self-attention-weights](../images/self-attention-weights.png)\n",
    "\n",
    "Finally, we compute the context vectors by multiplying the attention weights with the value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6ba7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights_2: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 /d_k**0.5, dim=-1)\n",
    "print(\"attn_weights_2:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc77ce",
   "metadata": {},
   "source": [
    "The rationale behind scaled-dot product attention\n",
    "\n",
    "\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7f46",
   "metadata": {},
   "source": [
    "Similar to when we computed the context vector as a weighted sum over the input vectors (see section 3.3), we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7ef70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vec_2: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"context_vec_2:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419f1eb",
   "metadata": {},
   "source": [
    "So far, we‚Äôve only computed a single context vector, z(2). Next, we will generalize the code to compute all context vectors in the input sequence, z(1) to z(T)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035aaff0",
   "metadata": {},
   "source": [
    "Why query, key, and value?\n",
    "\n",
    "\n",
    "The terms ‚Äúkey,‚Äù ‚Äúquery,‚Äù and ‚Äúvalue‚Äù in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
    "\n",
    "\n",
    "A query is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "\n",
    "The key is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query. \n",
    "\n",
    "\n",
    "The value in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fce101",
   "metadata": {},
   "source": [
    "## Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd711c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a compact self-attention class\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8517779",
   "metadata": {},
   "source": [
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models that provides necessary functionalities for model layer creation and management. \n",
    "\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out. \n",
    "\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8842cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a6506",
   "metadata": {},
   "source": [
    "As a quick check, notice that the second row ([0.3061, 0.8210]) matches the contents of context_vec_2 in the previous section.\n",
    "\n",
    "Self-attention involves the trainable weight matrices Wq, Wk, and Wv. These matrices transform input data into queries, keys, and values, respectively, which are crucial components of the attention mechanism. As the model is exposed to more data during training, it adjusts these trainable weights\n",
    "\n",
    "![self-attention-class](../images/self-attention-v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bc9aa",
   "metadata": {},
   "source": [
    "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch‚Äôs nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c2b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a self-attention class using PyTorch's nn.Linear layers\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "546b3ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760bea1",
   "metadata": {},
   "source": [
    "Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\n",
    "\n",
    "\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results. To check that both implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a SelfAttention_v1, such that both objects then produce the same results.\n",
    "\n",
    "\n",
    "Your task is to correctly assign the weights from an instance of SelfAttention_v2 to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight matrix in a transposed form.) After the assignment, you should observe that both instances produce the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd545798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before weight transfer:\n",
      "v1 output:\n",
      " tensor([[0.1671, 0.3726],\n",
      "        [0.1677, 0.3746]], grad_fn=<MmBackward0>)\n",
      "v2 output:\n",
      " tensor([[0.3038, 0.2414],\n",
      "        [0.3047, 0.2418]], grad_fn=<MmBackward0>)\n",
      "\n",
      "After weight transfer:\n",
      "v1 output:\n",
      " tensor([[0.3038, 0.2414],\n",
      "        [0.3047, 0.2418]], grad_fn=<MmBackward0>)\n",
      "v2 output:\n",
      " tensor([[0.3038, 0.2414],\n",
      "        [0.3047, 0.2418]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Outputs are equal: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# [Your class definitions here]\n",
    "\n",
    "# Create instances with same dimensions\n",
    "torch.manual_seed(123)\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.rand(2, d_in)\n",
    "\n",
    "# Before weight transfer - different outputs\n",
    "print(\"Before weight transfer:\")\n",
    "print(\"v1 output:\\n\", sa_v1(x))\n",
    "print(\"v2 output:\\n\", sa_v2(x))\n",
    "print()\n",
    "\n",
    "# Transfer weights from v2 to v1 (TRANSPOSE is the key!)\n",
    "sa_v1.W_query = nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key = nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_value = nn.Parameter(sa_v2.W_value.weight.T)\n",
    "\n",
    "# After weight transfer - identical outputs\n",
    "print(\"After weight transfer:\")\n",
    "print(\"v1 output:\\n\", sa_v1(x))\n",
    "print(\"v2 output:\\n\", sa_v2(x))\n",
    "print()\n",
    "\n",
    "# Verify they're equal\n",
    "print(\"Outputs are equal:\", torch.allclose(sa_v1(x), sa_v2(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56430d",
   "metadata": {},
   "source": [
    "please see https://claude.ai/chat/97eef876-1d16-4a7c-a8c3-2620c1de2c3a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b03c2",
   "metadata": {},
   "source": [
    "Next, we will make enhancements to the self-attention mechanism, focusing specifically on incorporating causal and multi-head elements. The causal aspect involves modifying the attention mechanism to prevent the model from accessing future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words. \n",
    "\n",
    "\n",
    "The multi-head component involves splitting the attention mechanism into multiple ‚Äúheads.‚Äù Each head learns different aspects of the data, allowing the model to simultaneously attend to information from different representation subspaces at different positions. This improves the model‚Äôs performance in complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cff00",
   "metadata": {},
   "source": [
    "## Hiding future words with causal attention\n",
    "\n",
    "For many LLM tasks, you will want the self-attention mechanism to consider only the tokens that appear prior to the current position when predicting the next token in a sequence. Causal attention, also known as masked attention, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n",
    "\n",
    "\n",
    "Now, we will modify the standard self-attention mechanism to create a causal attention mechanism, which is essential for developing an LLM in the subsequent chapters. To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text. We mask out the attention weights above the diagonal, and we normalize the nonmasked attention weights such that the attention weights sum to 1 in each row. Later, we will implement this masking and normalization procedure in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3180d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights from sa_v2:\n",
      " tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Attention weights from sa_v2:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcf4ad",
   "metadata": {},
   "source": [
    "We can implement the second step using PyTorch‚Äôs tril function to create a mask where the values above the diagonal are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b7c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple lower-triangular mask:\n",
      " tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(\"Simple lower-triangular mask:\\n\", mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b3a72",
   "metadata": {},
   "source": [
    "Now, we can multiply this mask with the attention weights to zero-out the values above the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ca13d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights (simple):\n",
      " tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(\"Masked attention weights (simple):\\n\", masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f951b7",
   "metadata": {},
   "source": [
    "The third step is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each element in each row by the sum in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5d09a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized masked attention weights (simple):\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_normalized = masked_simple / row_sums\n",
    "print(\"Normalized masked attention weights (simple):\\n\", masked_simple_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32a82a",
   "metadata": {},
   "source": [
    "The softmax function converts its inputs into a probability distribution. When negative infinity values (-‚àû) are present in a row, the softmax function treats them as zero probability. (Mathematically, this is because e‚Äâ‚Äâ‚Äì‚àû approaches 0.)\n",
    "\n",
    "\n",
    "We can implement this more efficient masking ‚Äútrick‚Äù by creating a mask with 1s above the diagonal and then replacing these 1s with negative infinity (-inf) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b12a7382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores (with -inf):\n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked attention scores (with -inf):\\n\", masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e3c5426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights after softmax with masking:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Attention weights after softmax with masking:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f66a5",
   "metadata": {},
   "source": [
    "We could now use the modified attention weights to compute the context vectors via `context_vec = attn_weights @ values`, as in section 3.4. However, we will first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d0d70f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors with masking:\n",
      " tensor([[-0.0872,  0.0286],\n",
      "        [-0.0991,  0.0501],\n",
      "        [-0.0999,  0.0633],\n",
      "        [-0.0983,  0.0489],\n",
      "        [-0.0514,  0.1098],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_vec = attn_weights @ sa_v2.W_value(inputs)\n",
    "print(\"Context vectors with masking:\\n\", context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab6612",
   "metadata": {},
   "source": [
    "## Masking additional attention weights with dropout\n",
    "\n",
    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively ‚Äúdropping‚Äù them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It‚Äôs important to emphasize that dropout is only used during training and is disabled afterward.\n",
    "\n",
    "\n",
    "In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we will apply the dropout mask after computing the attention weights, because it‚Äôs the more common variant in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04010047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # we choose a dropout rate of 50%\n",
    "\n",
    "example = torch.ones(6, 6)      # we create a matrix of 1s\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa0ba755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6162b4",
   "metadata": {},
   "source": [
    "## Implementing a compact causal attention class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e4cb0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Two inputs with six tokens each; each token has embedding dimension 3\n",
    "batch = torch.stack((inputs, inputs), dim=0) \n",
    "\n",
    "print(\"batch shape:\", batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35b518fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a compact causal attention class\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # add a dropout layer\n",
    "        \n",
    "        # create lower-triangular mask\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)   \n",
    "\n",
    "        # In PyTorch, operations with a trailing underscore are performed in-place, \n",
    "        # avoiding unnecessary memory copies.\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6cfd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs shape: torch.Size([2, 6, 2])\n",
      "context_vecs: tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs shape:\", context_vecs.shape)\n",
    "print(\"context_vecs:\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c8890",
   "metadata": {},
   "source": [
    "# Extending single-head attention to multi-head attention\n",
    "\n",
    "The term ‚Äúmulti-head‚Äù refers to dividing the attention mechanism into multiple ‚Äúheads,‚Äù each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
