{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e568c94",
   "metadata": {},
   "source": [
    "# Coding Attention Mechanisms\n",
    "\n",
    "- The reasons for using attention mechanisms in neural networks\n",
    "- A basic self-attention framework, progressing to an enhanced self-attention mechanism \n",
    "- A causal attention module that allows LLMs to generate one token at a time\n",
    "- Masking randomly selected attention weights with dropout to reduce overfitting\n",
    "- Stacking multiple causal attention modules into a multi-head attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bb970",
   "metadata": {},
   "source": [
    "Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder‚Äìdecoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9741434",
   "metadata": {},
   "source": [
    "In an encoder‚Äìdecoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, trying to capture the entire meaning of the input sentence in the final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a8e400",
   "metadata": {},
   "source": [
    "The decoder then takes this final hidden state to start generating the translated sentence, one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a7cca",
   "metadata": {},
   "source": [
    "Before the advent of transformer models, encoder‚Äìdecoder RNNs were a popular choice for machine translation. The encoder takes a sequence of tokens from the source language as input, where a hidden state (an intermediate neural network layer) of the encoder encodes a compressed representation of the entire input sequence. Then, the decoder uses its current hidden state to begin the translation, token by token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fafa5e",
   "metadata": {},
   "source": [
    "While we don‚Äôt need to know the inner workings of these encoder‚Äìdecoder RNNs, the key idea here is that the encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in this hidden state to produce the output. You can think of this hidden state as an embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea0609",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or ‚Äúattend to,‚Äù all other positions in the same sequence when computing the representation of a sequence. Self-attention is a key component of contemporary LLMs based on the transformer architecture, such as the GPT series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d373e9",
   "metadata": {},
   "source": [
    "# The meaning of \"self\"\n",
    "\n",
    "In self-attention, \"self\" refers to computing attention **within the same sequence**. Specifically:\n",
    "\n",
    "- Each element in the sequence establishes relationships with **all other elements in that same sequence** (including itself)\n",
    "- For example: when processing a sentence, each word attends to all other words in that sentence\n",
    "- \"Self\" emphasizes **attending to itself**, meaning the relationships are computed among elements within the input sequence itself\n",
    "\n",
    "**Example**: The sentence \"I love eating apples\"\n",
    "- The word \"apples\" will attend to \"I\", \"love\", \"eating\", and \"apples\" itself\n",
    "- All these relationships are established **within the same** input sentence\n",
    "\n",
    "## What are \"traditional attention mechanisms\"?\n",
    "\n",
    "Traditional attention mechanisms primarily refer to **attention used in sequence-to-sequence models**:\n",
    "\n",
    "- Attention is computed **between two different sequences**\n",
    "- Typical application: machine translation\n",
    "  - **Encoder sequence** (source language): English sentence\n",
    "  - **Decoder sequence** (target language): Chinese sentence\n",
    "  - Each Chinese character in the decoder attends to all English words in the encoder\n",
    "\n",
    "**Key difference**:\n",
    "- **Traditional attention**: Establishes relationships between two different sequences (Sequence A ‚Üí Sequence B)\n",
    "- **Self-attention**: Establishes relationships within a single sequence (among elements within Sequence A itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418aa30b",
   "metadata": {},
   "source": [
    "# A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf397fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33], # with     (x^4)\n",
    "    [0.77, 0.25, 0.10], # one      (x^5)\n",
    "    [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3c389",
   "metadata": {},
   "source": [
    "![attention-score](\"../images/attention-score.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a47018e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query vector: tensor([0.5500, 0.8700, 0.6600])\n",
      "inputs shape: 6\n",
      "attn_scores_2: tensor([0., 0., 0., 0., 0., 0.])\n",
      "attn_scores_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # the second input token served as query (journey)\n",
    "print(\"query vector:\", query)\n",
    "\n",
    "print(\"inputs shape:\", inputs.shape[0])\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(\"attn_scores_2:\", attn_scores_2)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(\"attn_scores_2:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5cee6",
   "metadata": {},
   "source": [
    "the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or ‚Äúattends to,‚Äù any other element: the higher the dot product, the higher the similarity and attention score between two elements.\n",
    "\n",
    "Raku code to compute attention scores using dot products:\n",
    "\n",
    "```raku\n",
    "# ÂÆö‰πâËæìÂÖ•Âº†ÈáèÔºà6‰∏™tokenÔºåÊØè‰∏™3Áª¥ÂêëÈáèÔºâ\n",
    "my @inputs = (\n",
    "    [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "    [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "    [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "    [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "    [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "    [0.05, 0.80, 0.55]   # step     (x^6)\n",
    ");\n",
    "\n",
    "# ÈÄâÊã©Á¨¨‰∫å‰∏™token‰Ωú‰∏∫Êü•ËØ¢ÂêëÈáèÔºàÁ¥¢Âºï‰∏∫1Ôºâ\n",
    "my @query = @inputs[1].flat;  # ‰ΩøÁî® .flat Â±ïÂºÄÊï∞ÁªÑ\n",
    "say \"query vector: [{@query.join(', ')}]\";\n",
    "say \"inputs shape: {@inputs.elems}\";\n",
    "\n",
    "# ÂàõÂª∫Á©∫Êï∞ÁªÑÂ≠òÂÇ®Ê≥®ÊÑèÂäõÂàÜÊï∞ÔºàRaku Êï∞ÁªÑ‰ºöËá™Âä®Êâ©Â±ïÔºâ\n",
    "my @attn_scores_2;\n",
    "say \"attn_scores_2: []\";\n",
    "\n",
    "# ‰ΩøÁî®Ë∂ÖËøêÁÆóÁ¨¶ ¬ª*¬´ Âíå reduction operator [+] ËÆ°ÁÆóÊØè‰∏™ËæìÂÖ•ÂêëÈáè‰∏éÊü•ËØ¢ÂêëÈáèÁöÑÁÇπÁßØ\n",
    "my @attn-scores = @inputs.map: -> @x { [+] @x ¬ª*¬´ @query };\n",
    "say \"attn_scores: [{@attn-scores.join(', ')}]\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe96eef",
   "metadata": {},
   "source": [
    "## normalization\n",
    "\n",
    "we normalize each of the attention scores we computed previously. The main goal behind the normalization is to obtain attention weights that sum up to 1. This normalization is a convention that is useful for interpretation and maintaining training stability in an LLM. Here‚Äôs a straightforward method for achieving this normalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13b165fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "sum of attention weights: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"attention weights:\", attn_weights_2_tmp)\n",
    "print(\"sum of attention weights:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a8030",
   "metadata": {},
   "source": [
    "```raku\n",
    "# ÂΩí‰∏ÄÂåñÊìç‰ΩúÔºöÂ∞ÜÊ≥®ÊÑèÂäõÂàÜÊï∞Èô§‰ª•ÊÄªÂíåÔºåÂæóÂà∞Ê≥®ÊÑèÂäõÊùÉÈáçÔºàÂíå‰∏∫1Ôºâ\n",
    "my $sum = [+] @attn-scores;\n",
    "my @attn_weights_2_tmp = @attn-scores ¬ª/¬ª $sum;\n",
    "say \"attention weights: {@attn_weights_2_tmp}\";\n",
    "say \"sum of attention weights: {[+] @attn_weights_2_tmp}\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289f5ab",
   "metadata": {},
   "source": [
    "In practice, it‚Äôs more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "093fd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights (naive softmax): tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum of attention weights (naive softmax): tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"attention weights (naive softmax):\", attn_weights_2_naive)\n",
    "print(\"sum of attention weights (naive softmax):\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e234704",
   "metadata": {},
   "source": [
    "the softmax function ensures that the attention weights are always positive. This makes the output interpretable as probabilities or relative importance, where higher weights indicate greater importance.\n",
    "\n",
    "Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it‚Äôs advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53cc94be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum of attention weights: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"attention weights:\", attn_weights_2)\n",
    "print(\"sum of attention weights:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a0a10",
   "metadata": {},
   "source": [
    "calculating the context vector z(2) by multiplying the embedded input tokens, x(i), with the corresponding attention weights and then summing the resulting vectors. Thus, context vector z(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1471e13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # the second input token served as query (journey)\n",
    "context_vec_2 = torch.zeros(inputs.shape[1])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(\"context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df18789",
   "metadata": {},
   "source": [
    "## Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a88a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores: tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# step1: compute attention scores for all queries\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for x, x_i in enumerate(inputs):\n",
    "    for i, x_j in enumerate(inputs):\n",
    "        attn_scores[x, i] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(\"attn_scores:\", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4bea51",
   "metadata": {},
   "source": [
    "Each element in the tensor represents an attention score between each pair of inputs.\n",
    "\n",
    "When computing the preceding attention score tensor, we used for loops in Python. However, for loops are generally slow, and we can achieve the same results using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "530fcb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores (matrix multiplication): tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(\"attn_scores (matrix multiplication):\", attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1cc82aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights: tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# step2: compute attention weights for all queries\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(\"attn_weights:\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbf57e",
   "metadata": {},
   "source": [
    "In the context of using PyTorch, the dim parameter in functions like `torch.softmax` specifies the dimension of the input tensor along which the function will be computed. By setting `dim=-1`, we are instructing the `softmax` function to apply the normalization along the last dimension of the attn_scores tensor. If attn_scores is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5fd8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 2 sum: 1.0\n",
      "all row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"row 2 sum:\", row_2_sum)\n",
    "print(\"all row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70fcf903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_context_vecs: tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# step3: compute context vectors for all queries\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(\"all_context_vecs:\", all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b47187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous context vector for query 2: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"previous context vector for query 2:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078d565",
   "metadata": {},
   "source": [
    "# Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b59a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]         # the second input element\n",
    "d_in = inputs.shape[1]  # the input embedding size, d_in=3\n",
    "d_out = 2               # the output embedding size, d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21d032ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight matrix Wq, Wk, and Wv\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842a9f6",
   "metadata": {},
   "source": [
    "We set `requires_grad=False` to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set `requires_grad=True` to update these matrices during model training.\n",
    "\n",
    "Next, we compute the query, key, and value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5d037d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_2: tensor([0.4306, 1.4551])\n",
      "key_2: tensor([0.4433, 1.1419])\n",
      "value_2: tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(\"query_2:\", query_2)\n",
    "print(\"key_2:\", key_2)\n",
    "print(\"value_2:\", value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282f55e",
   "metadata": {},
   "source": [
    "Weight parameters vs. attention weights \n",
    "\n",
    "\n",
    "In the weight matrices W, the term ‚Äúweight‚Äù is short for ‚Äúweight parameters,‚Äù the values of a neural network that are optimized during training. This is not to be confused with the attention weights. As we already saw, attention weights determine the extent to which a context vector depends on the different parts of the input (i.e., to what extent the network focuses on different parts of the input). \n",
    "\n",
    "\n",
    "In summary, weight parameters are the fundamental, learned coefficients that define the network‚Äôs connections, while attention weights are dynamic, context-specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74415ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95cb9a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores_22: tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(\"attn_scores_22:\", attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389ef8d",
   "metadata": {},
   "source": [
    "we can generalize this computation to all attention scores via matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a95e4f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores_2: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(\"attn_scores_2:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb12661",
   "metadata": {},
   "source": [
    "As we can see, as a quick check, the second element in the output matches the attn_score_22 we computed previously\n",
    "\n",
    "Now, we want to go from the attention scores to the attention weights, as illustrated in figure 3.16. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e4770",
   "metadata": {},
   "source": [
    "After computing the attention scores œâ, \n",
    "the next step is to normalize these scores using the softmax function to obtain the attention weights ùõº.\n",
    "\n",
    "![self-attention-weights](../images/self-attention-weights.png)\n",
    "\n",
    "Finally, we compute the context vectors by multiplying the attention weights with the value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6ba7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights_2: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 /d_k**0.5, dim=-1)\n",
    "print(\"attn_weights_2:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc77ce",
   "metadata": {},
   "source": [
    "The rationale behind scaled-dot product attention\n",
    "\n",
    "\n",
    "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "\n",
    "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7f46",
   "metadata": {},
   "source": [
    "Similar to when we computed the context vector as a weighted sum over the input vectors (see section 3.3), we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7ef70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vec_2: tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"context_vec_2:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419f1eb",
   "metadata": {},
   "source": [
    "So far, we‚Äôve only computed a single context vector, z(2). Next, we will generalize the code to compute all context vectors in the input sequence, z(1) to z(T)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035aaff0",
   "metadata": {},
   "source": [
    "Why query, key, and value?\n",
    "\n",
    "\n",
    "The terms ‚Äúkey,‚Äù ‚Äúquery,‚Äù and ‚Äúvalue‚Äù in the context of attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
    "\n",
    "\n",
    "A query is analogous to a search query in a database. It represents the current item (e.g., a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "\n",
    "The key is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query. \n",
    "\n",
    "\n",
    "The value in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
